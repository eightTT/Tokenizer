{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eightTT/Tokenizer/blob/main/Tokenizer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho79R1Vo7I4I"
      },
      "source": [
        "### Part1: Build simple Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXngJ2MwRScZ"
      },
      "source": [
        "this step test the converting text to raw byte\\\n",
        "this is the naive way to convert human language to language that machine can understand\\\n",
        "in the most naive way, we can continue with this and build a 'vocabulary', but\\\n",
        "this way has 2 big drawbacks: cost too much memory + bad for sematic prediction\\\n",
        "so\\\n",
        "we do smth like BytePairEncoding: encode text by Pair so we can shrink the memory cost and still keep the semantic quality\\\n",
        "so\\\n",
        "we train our Tokenizer on BPE algo. and this step is Separate from LLM training. this step is called Pre-processing stuff.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I8kWlsTf8Fb",
        "outputId": "b812a616-2f8e-448a-b754-52e6581f6ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "In the Symposium, Eros is recognized both as erotic lover and as a phenomenon capable of inspiring courage, valor, great deeds and works, and vanquishing man's natural fear of death. It is seen as transcending its earthly origins and attaining spiritual heights. The extraordinary elevation of the concept of love raises a question of whether some of the most extreme extents of meaning might be intended as humor or farce. Eros is almost always translated as \"love,\" and the English word has its own varieties and ambiguities that provide additional challenges to the effort to understand the Eros of ancient Athens.\n",
            "617\n",
            "---\n",
            "[73, 110, 32, 116, 104, 101, 32, 83, 121, 109, 112, 111, 115, 105, 117, 109, 44, 32, 69, 114, 111, 115, 32, 105, 115, 32, 114, 101, 99, 111, 103, 110, 105, 122, 101, 100, 32, 98, 111, 116, 104, 32, 97, 115, 32, 101, 114, 111, 116, 105, 99, 32, 108, 111, 118, 101, 114, 32, 97, 110, 100, 32, 97, 115, 32, 97, 32, 112, 104, 101, 110, 111, 109, 101, 110, 111, 110, 32, 99, 97, 112, 97, 98, 108, 101, 32, 111, 102, 32, 105, 110, 115, 112, 105, 114, 105, 110, 103, 32, 99, 111, 117, 114, 97, 103, 101, 44, 32, 118, 97, 108, 111, 114, 44, 32, 103, 114, 101, 97, 116, 32, 100, 101, 101, 100, 115, 32, 97, 110, 100, 32, 119, 111, 114, 107, 115, 44, 32, 97, 110, 100, 32, 118, 97, 110, 113, 117, 105, 115, 104, 105, 110, 103, 32, 109, 97, 110, 39, 115, 32, 110, 97, 116, 117, 114, 97, 108, 32, 102, 101, 97, 114, 32, 111, 102, 32, 100, 101, 97, 116, 104, 46, 32, 73, 116, 32, 105, 115, 32, 115, 101, 101, 110, 32, 97, 115, 32, 116, 114, 97, 110, 115, 99, 101, 110, 100, 105, 110, 103, 32, 105, 116, 115, 32, 101, 97, 114, 116, 104, 108, 121, 32, 111, 114, 105, 103, 105, 110, 115, 32, 97, 110, 100, 32, 97, 116, 116, 97, 105, 110, 105, 110, 103, 32, 115, 112, 105, 114, 105, 116, 117, 97, 108, 32, 104, 101, 105, 103, 104, 116, 115, 46, 32, 84, 104, 101, 32, 101, 120, 116, 114, 97, 111, 114, 100, 105, 110, 97, 114, 121, 32, 101, 108, 101, 118, 97, 116, 105, 111, 110, 32, 111, 102, 32, 116, 104, 101, 32, 99, 111, 110, 99, 101, 112, 116, 32, 111, 102, 32, 108, 111, 118, 101, 32, 114, 97, 105, 115, 101, 115, 32, 97, 32, 113, 117, 101, 115, 116, 105, 111, 110, 32, 111, 102, 32, 119, 104, 101, 116, 104, 101, 114, 32, 115, 111, 109, 101, 32, 111, 102, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 101, 120, 116, 114, 101, 109, 101, 32, 101, 120, 116, 101, 110, 116, 115, 32, 111, 102, 32, 109, 101, 97, 110, 105, 110, 103, 32, 109, 105, 103, 104, 116, 32, 98, 101, 32, 105, 110, 116, 101, 110, 100, 101, 100, 32, 97, 115, 32, 104, 117, 109, 111, 114, 32, 111, 114, 32, 102, 97, 114, 99, 101, 46, 32, 69, 114, 111, 115, 32, 105, 115, 32, 97, 108, 109, 111, 115, 116, 32, 97, 108, 119, 97, 121, 115, 32, 116, 114, 97, 110, 115, 108, 97, 116, 101, 100, 32, 97, 115, 32, 34, 108, 111, 118, 101, 44, 34, 32, 97, 110, 100, 32, 116, 104, 101, 32, 69, 110, 103, 108, 105, 115, 104, 32, 119, 111, 114, 100, 32, 104, 97, 115, 32, 105, 116, 115, 32, 111, 119, 110, 32, 118, 97, 114, 105, 101, 116, 105, 101, 115, 32, 97, 110, 100, 32, 97, 109, 98, 105, 103, 117, 105, 116, 105, 101, 115, 32, 116, 104, 97, 116, 32, 112, 114, 111, 118, 105, 100, 101, 32, 97, 100, 100, 105, 116, 105, 111, 110, 97, 108, 32, 99, 104, 97, 108, 108, 101, 110, 103, 101, 115, 32, 116, 111, 32, 116, 104, 101, 32, 101, 102, 102, 111, 114, 116, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 116, 104, 101, 32, 69, 114, 111, 115, 32, 111, 102, 32, 97, 110, 99, 105, 101, 110, 116, 32, 65, 116, 104, 101, 110, 115, 46]\n",
            "617\n"
          ]
        }
      ],
      "source": [
        "# source = \"https://en.wikipedia.org/wiki/Symposium_(Plato)\"\n",
        "text = \"In the Symposium, Eros is recognized both as erotic lover and as a phenomenon capable of inspiring courage, valor, great deeds and works, and vanquishing man's natural fear of death. It is seen as transcending its earthly origins and attaining spiritual heights. The extraordinary elevation of the concept of love raises a question of whether some of the most extreme extents of meaning might be intended as humor or farce. Eros is almost always translated as \\\"love,\\\" and the English word has its own varieties and ambiguities that provide additional challenges to the effort to understand the Eros of ancient Athens.\"\n",
        "tokens = text.encode(\"utf-8\") #raw bytes\n",
        "tokens = list(map(int, tokens)) #map integer value for bytes\n",
        "print('---')\n",
        "print(text)\n",
        "print(len(text))\n",
        "print('---')\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QPJvnbciFfF"
      },
      "source": [
        "Byte Pair Encoding (BPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "egWN8qPqkIZg"
      },
      "outputs": [],
      "source": [
        "def get_common_pair(ids:list):\n",
        "  '''\n",
        "  the function that find the most common pair in the text corpus,\n",
        "  read the list of ids,\n",
        "  create (a dictionary of) pair of byte,\n",
        "  count to keep tract of total number (not neccesary),\n",
        "  return dictionary not sorted, pair is distinct\n",
        "  '''\n",
        "  counts = {}\n",
        "  for pair in zip(ids, ids[1:]):\n",
        "    counts[pair] = counts.get(pair, 0) + 1\n",
        "  return counts\n",
        "\n",
        "stats = get_common_pair(tokens)\n",
        "# print(stats)\n",
        "# print(sorted(( (v,k) for k, v in stats.items()), reverse=True))\n",
        "# chr(115),chr(32),chr(97),chr(110)\n",
        "# top_pair = max(stats, key=stats.get)\n",
        "# top_pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MifAm_kqqIbx"
      },
      "source": [
        "the initial tokens have the ids from 0-255\\\n",
        "for new tokens (the pair), assign id = 256\n",
        "and so on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6R00gInqnUe",
        "outputId": "214152f6-81ac-4621-ad14-4d7c790f3559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[73, 110, 32, 116, 104, 101, 32, 83, 121, 109, 112, 111, 115, 105, 117, 109, 44, 32, 69, 114, 111, 256, 105, 256, 114, 101, 99, 111, 103, 110, 105, 122, 101, 100, 32, 98, 111, 116, 104, 32, 97, 256, 101, 114, 111, 116, 105, 99, 32, 108, 111, 118, 101, 114, 32, 97, 110, 100, 32, 97, 256, 97, 32, 112, 104, 101, 110, 111, 109, 101, 110, 111, 110, 32, 99, 97, 112, 97, 98, 108, 101, 32, 111, 102, 32, 105, 110, 115, 112, 105, 114, 105, 110, 103, 32, 99, 111, 117, 114, 97, 103, 101, 44, 32, 118, 97, 108, 111, 114, 44, 32, 103, 114, 101, 97, 116, 32, 100, 101, 101, 100, 256, 97, 110, 100, 32, 119, 111, 114, 107, 115, 44, 32, 97, 110, 100, 32, 118, 97, 110, 113, 117, 105, 115, 104, 105, 110, 103, 32, 109, 97, 110, 39, 256, 110, 97, 116, 117, 114, 97, 108, 32, 102, 101, 97, 114, 32, 111, 102, 32, 100, 101, 97, 116, 104, 46, 32, 73, 116, 32, 105, 256, 115, 101, 101, 110, 32, 97, 256, 116, 114, 97, 110, 115, 99, 101, 110, 100, 105, 110, 103, 32, 105, 116, 256, 101, 97, 114, 116, 104, 108, 121, 32, 111, 114, 105, 103, 105, 110, 256, 97, 110, 100, 32, 97, 116, 116, 97, 105, 110, 105, 110, 103, 32, 115, 112, 105, 114, 105, 116, 117, 97, 108, 32, 104, 101, 105, 103, 104, 116, 115, 46, 32, 84, 104, 101, 32, 101, 120, 116, 114, 97, 111, 114, 100, 105, 110, 97, 114, 121, 32, 101, 108, 101, 118, 97, 116, 105, 111, 110, 32, 111, 102, 32, 116, 104, 101, 32, 99, 111, 110, 99, 101, 112, 116, 32, 111, 102, 32, 108, 111, 118, 101, 32, 114, 97, 105, 115, 101, 256, 97, 32, 113, 117, 101, 115, 116, 105, 111, 110, 32, 111, 102, 32, 119, 104, 101, 116, 104, 101, 114, 32, 115, 111, 109, 101, 32, 111, 102, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 101, 120, 116, 114, 101, 109, 101, 32, 101, 120, 116, 101, 110, 116, 256, 111, 102, 32, 109, 101, 97, 110, 105, 110, 103, 32, 109, 105, 103, 104, 116, 32, 98, 101, 32, 105, 110, 116, 101, 110, 100, 101, 100, 32, 97, 256, 104, 117, 109, 111, 114, 32, 111, 114, 32, 102, 97, 114, 99, 101, 46, 32, 69, 114, 111, 256, 105, 256, 97, 108, 109, 111, 115, 116, 32, 97, 108, 119, 97, 121, 256, 116, 114, 97, 110, 115, 108, 97, 116, 101, 100, 32, 97, 256, 34, 108, 111, 118, 101, 44, 34, 32, 97, 110, 100, 32, 116, 104, 101, 32, 69, 110, 103, 108, 105, 115, 104, 32, 119, 111, 114, 100, 32, 104, 97, 256, 105, 116, 256, 111, 119, 110, 32, 118, 97, 114, 105, 101, 116, 105, 101, 256, 97, 110, 100, 32, 97, 109, 98, 105, 103, 117, 105, 116, 105, 101, 256, 116, 104, 97, 116, 32, 112, 114, 111, 118, 105, 100, 101, 32, 97, 100, 100, 105, 116, 105, 111, 110, 97, 108, 32, 99, 104, 97, 108, 108, 101, 110, 103, 101, 256, 116, 111, 32, 116, 104, 101, 32, 101, 102, 102, 111, 114, 116, 32, 116, 111, 32, 117, 110, 100, 101, 114, 115, 116, 97, 110, 100, 32, 116, 104, 101, 32, 69, 114, 111, 256, 111, 102, 32, 97, 110, 99, 105, 101, 110, 116, 32, 65, 116, 104, 101, 110, 115, 46]\n",
            "594\n"
          ]
        }
      ],
      "source": [
        "def map_new_token_with_new_id(ids, pair, idx):\n",
        "  '''\n",
        "  get the list of id (ids)\n",
        "  the maximum pair from the ids (pair)\n",
        "  repalce by a new index (idx)\n",
        "  '''\n",
        "  new_ids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids)-1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      new_ids.append(idx)\n",
        "      i+=2\n",
        "    else:\n",
        "      new_ids.append(ids[i])\n",
        "      i+=1\n",
        "  return new_ids\n",
        "\n",
        "token2 = map_new_token_with_new_id(tokens, top_pair, 256)\n",
        "print(token2)\n",
        "print(len(token2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqtGh8oHupOH"
      },
      "source": [
        "the more we merge, the bigger the vocablary is, shorter sequences\\\n",
        "tune this to get the best vocabulary size (token size) \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkT6-Tyy6lib"
      },
      "source": [
        "#### Step: train (build) the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DLzMmc0WVz5",
        "outputId": "006291f7-bb70-451e-c919-94d9f2f7f9ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "merge pair (115, 32) into new id 256\n",
            "merge pair (101, 32) into new id 257\n",
            "merge pair (97, 110) into new id 258\n",
            "merge pair (116, 104) into new id 259\n",
            "merge pair (100, 32) into new id 260\n",
            "merge pair (105, 110) into new id 261\n",
            "merge pair (101, 110) into new id 262\n",
            "merge pair (116, 32) into new id 263\n",
            "merge pair (111, 102) into new id 264\n",
            "merge pair (264, 32) into new id 265\n",
            "merge pair (111, 114) into new id 266\n",
            "merge pair (258, 260) into new id 267\n",
            "merge pair (97, 108) into new id 268\n",
            "merge pair (259, 257) into new id 269\n",
            "merge pair (97, 256) into new id 270\n",
            "merge pair (116, 105) into new id 271\n",
            "merge pair (110, 32) into new id 272\n",
            "merge pair (114, 111) into new id 273\n",
            "merge pair (261, 103) into new id 274\n",
            "merge pair (274, 32) into new id 275\n",
            "merge pair (97, 114) into new id 276\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 276 # the desired final vocab size : we preset\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens) # we dont destroy the original list\n",
        "\n",
        "# declare a dictionary\n",
        "merges = {} #(int1, int2) -> int3\n",
        "\n",
        "# merge dict is maintain child1(int1), child2(int2) maping to a new tokens(int3)\n",
        "i = 0\n",
        "while i <= num_merges:\n",
        "  stats = get_common_pair(ids)\n",
        "  top_pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"merge pair {top_pair} into new id {idx}\")\n",
        "  ids = map_new_token_with_new_id(ids, top_pair, idx)\n",
        "  merges[top_pair] = idx\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dPg2HDzZd3U",
        "outputId": "8b6ee7e9-ebfe-4072-a74d-6096475e865e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ori tokens lenght: 617\n",
            "new tokens lenght: 436\n",
            "compression ratio: 1.42X\n"
          ]
        }
      ],
      "source": [
        "#statistic check\n",
        "print(\"ori tokens lenght:\", len(tokens))\n",
        "print(\"new tokens lenght:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens)/len(ids):.2f}X\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcOaV8x6LSeI",
        "outputId": "1ebb5b2d-4539-4879-d747-824825993bf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab:  {0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b's ', 257: b'e ', 258: b'an', 259: b'th', 260: b'd ', 261: b'in', 262: b'en', 263: b't ', 264: b'of', 265: b'of ', 266: b'or', 267: b'and ', 268: b'al', 269: b'the ', 270: b'as ', 271: b'ti', 272: b'n ', 273: b'ro', 274: b'ing', 275: b'ing ', 276: b'ar'}\n",
            "merges:  {(115, 32): 256, (101, 32): 257, (97, 110): 258, (116, 104): 259, (100, 32): 260, (105, 110): 261, (101, 110): 262, (116, 32): 263, (111, 102): 264, (264, 32): 265, (111, 114): 266, (258, 260): 267, (97, 108): 268, (259, 257): 269, (97, 256): 270, (116, 105): 271, (110, 32): 272, (114, 111): 273, (261, 103): 274, (274, 32): 275, (97, 114): 276}\n"
          ]
        }
      ],
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)} # init the raw byte dictionary\n",
        "\n",
        "for (p0, p1), idx in merges.items():         # mapping for the new token id in 'merges' to the byte object\n",
        "  vocab[idx] = vocab[p0] + vocab[p1]\n",
        "print('vocab: ', vocab)                       # new extended vocab\n",
        "print('merges: ', merges)                     # new extended merges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU7V07d0blj-"
      },
      "source": [
        "here come to the stage of **training** Tokenizer. \\\n",
        "we find the best size of vocabluary -> which will be store on Disk for later LLM use to learn \\\n",
        "we want to make sure it cover not just English \\\n",
        "and also code \\\n",
        "and maybe pixel in the future \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-urcMO_kR_K"
      },
      "source": [
        "Step: Decoding function\n",
        "Givem a sequence of integers in range [0, a], what are the text ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MEG_nV58kRCo",
        "outputId": "9949848b-1ba0-4731-bf5b-43783e137eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab:  {0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b's ', 257: b'e ', 258: b'an', 259: b'th', 260: b'd ', 261: b'in', 262: b'en', 263: b't ', 264: b'of', 265: b'of ', 266: b'or', 267: b'and ', 268: b'al', 269: b'the ', 270: b'as ', 271: b'ti', 272: b'n ', 273: b'ro', 274: b'ing', 275: b'ing ', 276: b'ar'}\n",
            "decoded output: In the Symposium, Eros is recognized both as erotic lover and as a phenomenon capable of inspiring courage, valor, great deeds and works, and vanquishing man's natural fear of death. It is seen as transcending its earthly origins and attaining spiritual heights. The extraordinary elevation of the concept of love raises a question of whether some of the most extreme extents of meaning might be intended as humor or farce. Eros is almost always translated as \"love,\" and the English word has its own varieties and ambiguities that provide additional challenges to the effort to understand the Eros of ancient Athens.\n"
          ]
        }
      ],
      "source": [
        "def decode(ids:list, vocab:dict):\n",
        "  '''\n",
        "  decode the sequence of integers to human text\n",
        "  using vocabulary that created before\n",
        "  output: list of string\n",
        "  '''\n",
        "  decoded_list = []\n",
        "  print('tokens:',ids)\n",
        "  for i in range(len(ids)):\n",
        "    token = vocab[ids[i]].decode(\"utf-8\", errors = 'replace')\n",
        "    decoded_list.append(token)\n",
        "  text = ''.join(decoded_list)\n",
        "  return text\n",
        "\n",
        "# def decode(ids:list):\n",
        "#   tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "#   text = tokens.decode(\"utf-8\", errors = 'replace')\n",
        "#   return text\n",
        "\n",
        "a = decode(ids)\n",
        "print('decoded output:',a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoRbfqCH3uVI",
        "outputId": "669f5573-26fb-4572-c0cb-445ac45342d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "436\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[72, 101, 108, 108, 111, 32, 119, 266, 108, 100, 115, 33]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def encode(text:str):\n",
        "  '''\n",
        "  encode the human text to sequence of integers\n",
        "  integer is map to byte object, from vocabulary\n",
        "  output: list of integers\n",
        "  '''\n",
        "  tokens = list(text.encode(\"utf-8\"))\n",
        "  while len(tokens) >= 2:\n",
        "    stats = get_common_pair(tokens)\n",
        "    pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n",
        "    if pair not in merges:\n",
        "      break\n",
        "      print('Nothing else can be merge!')\n",
        "    idx = merges[pair]\n",
        "    tokens = map_new_token_with_new_id(tokens, pair, idx)\n",
        "  return tokens\n",
        "a = encode(text)\n",
        "print(len(a))\n",
        "encode('Hello worlds!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KaDM9Wh_OHl",
        "outputId": "52424799-4532-4c40-ca5e-07b95945e513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello worlds!\n"
          ]
        }
      ],
      "source": [
        "print(decode(encode('Hello worlds!')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh-ue5tb66__"
      },
      "source": [
        "### Part2: Break down state-of-the-art tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6V5tA--7rYC"
      },
      "source": [
        "#### Forced splits using regex patterns : GPT series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LijlSgaW7mKS",
        "outputId": "5c5e8e6e-6d0a-4cd7-d646-6693937991a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ' world', \"'ve\", ' how', '?', ' are', ' you', ' 11']\n"
          ]
        }
      ],
      "source": [
        "# if string in text fall into these regex, consider it as a new token\n",
        "# this apply before BPE\n",
        "# highlevel explain: it break down text and run BPE  in each sub text then concat. this optimize the BPE and avoid random\n",
        "import regex as re\n",
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "print(re.findall(gpt2pat, \"Hello world've how? are you 11\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb4rydvmFrCt"
      },
      "source": [
        "#### Titoken library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXPF_qbTFuYa",
        "outputId": "9a1ccb97-62d1-4c5a-b448-cc48f805a9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[220, 220, 220, 18435, 995, 1053]\n",
            "[262, 22691, 1917, 3077]\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "#GPT-2\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "print(enc.encode(\"    Hello world've\"))\n",
        "\n",
        "#GPT-4\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "print(enc.encode(\"    Hello world've\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR1aAOJLIl5R"
      },
      "source": [
        "Special token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "anvZv8pVIu7r"
      },
      "outputs": [],
      "source": [
        "# !wget https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/vocab.bpe\n",
        "# !wget https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/encoder.json\n",
        "import os\n",
        "import json\n",
        "with open('encoder.json', 'r') as f:\n",
        "  encoder = json.load(f)            #<----equivalent to \"vocab\"\n",
        "\n",
        "with open('vocab.bpe', 'r', encoding = (\"utf-8\")) as f:\n",
        "  bpe_data = f.read()\n",
        "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]   #<---equivalent to \"merges\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AtXft9DOkYc",
        "outputId": "53d4134f-e6b0-4048-81fa-6c6ab344c9da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(encoder)   #256 raw byte + 50k merges + 1 special token\n",
        "encoder['<|endoftext|>']        #<---- this is special token\n",
        "# this token will not go to BPE merges\n",
        "# this will be handle by a special handler\n",
        "# use to end a document or a conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test BasicTokenizer module "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building BPE merges...\n",
            "merge pair (101, 32) into new id 256\n",
            "merge pair (115, 32) into new id 257\n",
            "merge pair (116, 104) into new id 258\n",
            "merge pair (110, 32) into new id 259\n",
            "merge pair (100, 32) into new id 260\n",
            "merge pair (97, 116) into new id 261\n",
            "merge pair (258, 256) into new id 262\n",
            "merge pair (44, 32) into new id 263\n",
            "merge pair (97, 110) into new id 264\n",
            "merge pair (101, 114) into new id 265\n",
            "merge pair (116, 32) into new id 266\n",
            "merge pair (105, 110) into new id 267\n",
            "merge pair (102, 32) into new id 268\n",
            "merge pair (111, 268) into new id 269\n",
            "merge pair (111, 32) into new id 270\n",
            "merge pair (121, 32) into new id 271\n",
            "merge pair (114, 101) into new id 272\n",
            "merge pair (97, 108) into new id 273\n",
            "merge pair (111, 114) into new id 274\n",
            "merge pair (111, 110) into new id 275\n",
            "merge pair (101, 110) into new id 276\n",
            "merge pair (104, 105) into new id 277\n",
            "merge pair (97, 114) into new id 278\n",
            "merge pair (101, 257) into new id 279\n",
            "merge pair (101, 260) into new id 280\n",
            "merge pair (46, 32) into new id 281\n",
            "merge pair (116, 105) into new id 282\n",
            "merge pair (264, 260) into new id 283\n",
            "merge pair (101, 115) into new id 284\n",
            "merge pair (116, 270) into new id 285\n",
            "merge pair (97, 32) into new id 286\n",
            "merge pair (105, 259) into new id 287\n",
            "merge pair (10, 10) into new id 288\n",
            "merge pair (267, 103) into new id 289\n",
            "merge pair (111, 117) into new id 290\n",
            "merge pair (111, 115) into new id 291\n",
            "merge pair (99, 104) into new id 292\n",
            "merge pair (105, 115) into new id 293\n",
            "merge pair (111, 259) into new id 294\n",
            "merge pair (111, 109) into new id 295\n",
            "merge pair (289, 32) into new id 296\n",
            "merge pair (105, 257) into new id 297\n",
            "merge pair (104, 256) into new id 298\n",
            "merge pair (97, 257) into new id 299\n",
            "merge pair (111, 118) into new id 300\n",
            "merge pair (265, 32) into new id 301\n",
            "merge pair (105, 99) into new id 302\n",
            "merge pair (261, 32) into new id 303\n",
            "merge pair (108, 261) into new id 304\n",
            "merge pair (99, 114) into new id 305\n",
            "merge pair (97, 115) into new id 306\n",
            "merge pair (101, 259) into new id 307\n",
            "merge pair (101, 108) into new id 308\n",
            "merge pair (117, 109) into new id 309\n",
            "merge pair (100, 105) into new id 310\n",
            "merge pair (116, 114) into new id 311\n",
            "merge pair (109, 112) into new id 312\n",
            "merge pair (273, 32) into new id 313\n",
            "merge pair (97, 259) into new id 314\n",
            "merge pair (111, 112) into new id 315\n",
            "merge pair (99, 105) into new id 316\n",
            "merge pair (258, 303) into new id 317\n",
            "merge pair (277, 257) into new id 318\n",
            "merge pair (119, 104) into new id 319\n",
            "merge pair (112, 101) into new id 320\n",
            "merge pair (258, 32) into new id 321\n",
            "merge pair (98, 101) into new id 322\n",
            "merge pair (97, 100) into new id 323\n",
            "merge pair (111, 305) into new id 324\n",
            "merge pair (105, 116) into new id 325\n",
            "merge pair (324, 261) into new id 326\n",
            "merge pair (83, 326) into new id 327\n",
            "merge pair (105, 114) into new id 328\n",
            "merge pair (108, 300) into new id 329\n",
            "merge pair (117, 114) into new id 330\n",
            "merge pair (269, 262) into new id 331\n",
            "merge pair (111, 119) into new id 332\n",
            "merge pair (97, 99) into new id 333\n",
            "merge pair (117, 115) into new id 334\n",
            "merge pair (288, 10) into new id 335\n",
            "merge pair (274, 32) into new id 336\n",
            "merge pair (101, 120) into new id 337\n",
            "merge pair (111, 108) into new id 338\n",
            "merge pair (80, 304) into new id 339\n",
            "merge pair (276, 116) into new id 340\n",
            "merge pair (115, 116) into new id 341\n",
            "merge pair (119, 105) into new id 342\n",
            "merge pair (99, 275) into new id 343\n",
            "merge pair (291, 105) into new id 344\n",
            "merge pair (108, 271) into new id 345\n",
            "merge pair (46, 335) into new id 346\n",
            "merge pair (292, 32) into new id 347\n",
            "merge pair (121, 312) into new id 348\n",
            "merge pair (104, 97) into new id 349\n",
            "merge pair (348, 344) into new id 350\n",
            "merge pair (100, 101) into new id 351\n",
            "merge pair (84, 298) into new id 352\n",
            "merge pair (350, 309) into new id 353\n",
            "merge pair (258, 101) into new id 354\n",
            "merge pair (263, 283) into new id 355\n",
            "merge pair (119, 299) into new id 356\n",
            "merge pair (226, 128) into new id 357\n",
            "merge pair (99, 295) into new id 358\n",
            "merge pair (114, 111) into new id 359\n",
            "merge pair (39, 257) into new id 360\n",
            "merge pair (98, 105) into new id 361\n",
            "merge pair (108, 101) into new id 362\n",
            "merge pair (103, 97) into new id 363\n",
            "merge pair (327, 279) into new id 364\n",
            "merge pair (98, 271) into new id 365\n",
            "merge pair (115, 320) into new id 366\n",
            "merge pair (108, 105) into new id 367\n",
            "merge pair (65, 108) into new id 368\n",
            "merge pair (342, 321) into new id 369\n",
            "merge pair (103, 117) into new id 370\n",
            "merge pair (261, 105) into new id 371\n",
            "merge pair (112, 272) into new id 372\n",
            "merge pair (256, 269) into new id 373\n",
            "merge pair (117, 257) into new id 374\n",
            "merge pair (315, 104) into new id 375\n",
            "merge pair (263, 262) into new id 376\n",
            "merge pair (97, 109) into new id 377\n",
            "merge pair (83, 353) into new id 378\n",
            "merge pair (361, 323) into new id 379\n",
            "merge pair (282, 99) into new id 380\n",
            "merge pair (101, 100) into new id 381\n",
            "merge pair (118, 256) into new id 382\n",
            "merge pair (368, 316) into new id 383\n",
            "merge pair (383, 379) into new id 384\n",
            "merge pair (103, 104) into new id 385\n",
            "merge pair (116, 265) into new id 386\n",
            "merge pair (302, 32) into new id 387\n",
            "merge pair (116, 257) into new id 388\n",
            "merge pair (113, 117) into new id 389\n",
            "merge pair (101, 118) into new id 390\n",
            "merge pair (117, 110) into new id 391\n",
            "merge pair (287, 262) into new id 392\n",
            "merge pair (111, 100) into new id 393\n",
            "merge pair (98, 256) into new id 394\n",
            "merge pair (295, 32) into new id 395\n",
            "merge pair (102, 114) into new id 396\n",
            "merge pair (97, 103) into new id 397\n",
            "merge pair (97, 117) into new id 398\n",
            "merge pair (105, 266) into new id 399\n",
            "merge pair (98, 108) into new id 400\n",
            "merge pair (277, 108) into new id 401\n",
            "merge pair (339, 111) into new id 402\n",
            "merge pair (366, 101) into new id 403\n",
            "merge pair (101, 281) into new id 404\n",
            "merge pair (109, 97) into new id 405\n",
            "merge pair (109, 32) into new id 406\n",
            "merge pair (280, 262) into new id 407\n",
            "merge pair (115, 105) into new id 408\n",
            "merge pair (73, 259) into new id 409\n",
            "merge pair (101, 263) into new id 410\n",
            "merge pair (102, 336) into new id 411\n",
            "merge pair (319, 270) into new id 412\n",
            "merge pair (117, 108) into new id 413\n",
            "merge pair (276, 266) into new id 414\n",
            "merge pair (310, 273) into new id 415\n",
            "merge pair (115, 263) into new id 416\n",
            "merge pair (112, 278) into new id 417\n",
            "merge pair (100, 284) into new id 418\n",
            "merge pair (111, 370) into new id 419\n",
            "merge pair (110, 111) into new id 420\n",
            "merge pair (329, 256) into new id 421\n",
            "merge pair (119, 277) into new id 422\n",
            "merge pair (415, 419) into new id 423\n",
            "merge pair (99, 108) into new id 424\n",
            "merge pair (115, 101) into new id 425\n",
            "merge pair (101, 99) into new id 426\n",
            "merge pair (422, 347) into new id 427\n",
            "merge pair (98, 117) into new id 428\n",
            "merge pair (112, 108) into new id 429\n",
            "merge pair (293, 116) into new id 430\n",
            "merge pair (396, 395) into new id 431\n",
            "merge pair (112, 114) into new id 432\n",
            "merge pair (384, 279) into new id 433\n",
            "merge pair (99, 256) into new id 434\n",
            "merge pair (401, 291) into new id 435\n",
            "merge pair (371, 294) into new id 436\n",
            "merge pair (258, 301) into new id 437\n",
            "merge pair (100, 265) into new id 438\n",
            "merge pair (265, 256) into new id 439\n",
            "merge pair (115, 117) into new id 440\n",
            "merge pair (108, 32) into new id 441\n",
            "merge pair (115, 270) into new id 442\n",
            "merge pair (105, 118) into new id 443\n",
            "merge pair (116, 97) into new id 444\n",
            "merge pair (112, 435) into new id 445\n",
            "merge pair (65, 114) into new id 446\n",
            "merge pair (278, 256) into new id 447\n",
            "merge pair (282, 294) into new id 448\n",
            "merge pair (282, 275) into new id 449\n",
            "merge pair (102, 274) into new id 450\n",
            "merge pair (108, 260) into new id 451\n",
            "merge pair (112, 111) into new id 452\n",
            "merge pair (378, 32) into new id 453\n",
            "merge pair (65, 363) into new id 454\n",
            "merge pair (420, 266) into new id 455\n",
            "merge pair (275, 256) into new id 456\n",
            "merge pair (349, 260) into new id 457\n",
            "merge pair (428, 266) into new id 458\n",
            "merge pair (97, 121) into new id 459\n",
            "merge pair (115, 112) into new id 460\n",
            "merge pair (101, 112) into new id 461\n",
            "merge pair (285, 262) into new id 462\n",
            "merge pair (454, 258) into new id 463\n",
            "merge pair (101, 109) into new id 464\n",
            "merge pair (266, 269) into new id 465\n",
            "merge pair (403, 292) into new id 466\n",
            "merge pair (281, 352) into new id 467\n",
            "merge pair (119, 274) into new id 468\n",
            "merge pair (264, 115) into new id 469\n",
            "merge pair (284, 263) into new id 470\n",
            "merge pair (290, 110) into new id 471\n",
            "merge pair (116, 280) into new id 472\n",
            "merge pair (115, 308) into new id 473\n",
            "merge pair (402, 360) into new id 474\n",
            "merge pair (290, 451) into new id 475\n",
            "merge pair (107, 32) into new id 476\n",
            "merge pair (116, 119) into new id 477\n",
            "merge pair (105, 109) into new id 478\n",
            "merge pair (446, 430) into new id 479\n",
            "merge pair (39, 32) into new id 480\n",
            "merge pair (68, 105) into new id 481\n",
            "merge pair (101, 278) into new id 482\n",
            "merge pair (463, 294) into new id 483\n",
            "merge pair (121, 263) into new id 484\n",
            "merge pair (290, 266) into new id 485\n",
            "merge pair (281, 72) into new id 486\n",
            "merge pair (282, 109) into new id 487\n",
            "merge pair (305, 105) into new id 488\n",
            "merge pair (339, 275) into new id 489\n",
            "merge pair (112, 32) into new id 490\n",
            "merge pair (354, 271) into new id 491\n",
            "merge pair (97, 98) into new id 492\n",
            "merge pair (372, 115) into new id 493\n",
            "merge pair (277, 109) into new id 494\n",
            "merge pair (276, 100) into new id 495\n",
            "merge pair (375, 264) into new id 496\n",
            "merge pair (311, 469) into new id 497\n",
            "merge pair (291, 266) into new id 498\n",
            "merge pair (101, 266) into new id 499\n",
            "merge pair (97, 271) into new id 500\n",
            "merge pair (116, 301) into new id 501\n",
            "merge pair (276, 99) into new id 502\n",
            "merge pair (258, 297) into new id 503\n",
            "merge pair (101, 97) into new id 504\n",
            "merge pair (76, 300) into new id 505\n",
            "merge pair (273, 442) into new id 506\n",
            "merge pair (261, 280) into new id 507\n",
            "merge pair (114, 290) into new id 508\n",
            "merge pair (445, 375) into new id 509\n",
            "merge pair (380, 32) into new id 510\n",
            "merge pair (293, 104) into new id 511\n",
            "merge pair (117, 313) into new id 512\n",
            "Complete merges:  {(101, 32): 256, (115, 32): 257, (116, 104): 258, (110, 32): 259, (100, 32): 260, (97, 116): 261, (258, 256): 262, (44, 32): 263, (97, 110): 264, (101, 114): 265, (116, 32): 266, (105, 110): 267, (102, 32): 268, (111, 268): 269, (111, 32): 270, (121, 32): 271, (114, 101): 272, (97, 108): 273, (111, 114): 274, (111, 110): 275, (101, 110): 276, (104, 105): 277, (97, 114): 278, (101, 257): 279, (101, 260): 280, (46, 32): 281, (116, 105): 282, (264, 260): 283, (101, 115): 284, (116, 270): 285, (97, 32): 286, (105, 259): 287, (10, 10): 288, (267, 103): 289, (111, 117): 290, (111, 115): 291, (99, 104): 292, (105, 115): 293, (111, 259): 294, (111, 109): 295, (289, 32): 296, (105, 257): 297, (104, 256): 298, (97, 257): 299, (111, 118): 300, (265, 32): 301, (105, 99): 302, (261, 32): 303, (108, 261): 304, (99, 114): 305, (97, 115): 306, (101, 259): 307, (101, 108): 308, (117, 109): 309, (100, 105): 310, (116, 114): 311, (109, 112): 312, (273, 32): 313, (97, 259): 314, (111, 112): 315, (99, 105): 316, (258, 303): 317, (277, 257): 318, (119, 104): 319, (112, 101): 320, (258, 32): 321, (98, 101): 322, (97, 100): 323, (111, 305): 324, (105, 116): 325, (324, 261): 326, (83, 326): 327, (105, 114): 328, (108, 300): 329, (117, 114): 330, (269, 262): 331, (111, 119): 332, (97, 99): 333, (117, 115): 334, (288, 10): 335, (274, 32): 336, (101, 120): 337, (111, 108): 338, (80, 304): 339, (276, 116): 340, (115, 116): 341, (119, 105): 342, (99, 275): 343, (291, 105): 344, (108, 271): 345, (46, 335): 346, (292, 32): 347, (121, 312): 348, (104, 97): 349, (348, 344): 350, (100, 101): 351, (84, 298): 352, (350, 309): 353, (258, 101): 354, (263, 283): 355, (119, 299): 356, (226, 128): 357, (99, 295): 358, (114, 111): 359, (39, 257): 360, (98, 105): 361, (108, 101): 362, (103, 97): 363, (327, 279): 364, (98, 271): 365, (115, 320): 366, (108, 105): 367, (65, 108): 368, (342, 321): 369, (103, 117): 370, (261, 105): 371, (112, 272): 372, (256, 269): 373, (117, 257): 374, (315, 104): 375, (263, 262): 376, (97, 109): 377, (83, 353): 378, (361, 323): 379, (282, 99): 380, (101, 100): 381, (118, 256): 382, (368, 316): 383, (383, 379): 384, (103, 104): 385, (116, 265): 386, (302, 32): 387, (116, 257): 388, (113, 117): 389, (101, 118): 390, (117, 110): 391, (287, 262): 392, (111, 100): 393, (98, 256): 394, (295, 32): 395, (102, 114): 396, (97, 103): 397, (97, 117): 398, (105, 266): 399, (98, 108): 400, (277, 108): 401, (339, 111): 402, (366, 101): 403, (101, 281): 404, (109, 97): 405, (109, 32): 406, (280, 262): 407, (115, 105): 408, (73, 259): 409, (101, 263): 410, (102, 336): 411, (319, 270): 412, (117, 108): 413, (276, 266): 414, (310, 273): 415, (115, 263): 416, (112, 278): 417, (100, 284): 418, (111, 370): 419, (110, 111): 420, (329, 256): 421, (119, 277): 422, (415, 419): 423, (99, 108): 424, (115, 101): 425, (101, 99): 426, (422, 347): 427, (98, 117): 428, (112, 108): 429, (293, 116): 430, (396, 395): 431, (112, 114): 432, (384, 279): 433, (99, 256): 434, (401, 291): 435, (371, 294): 436, (258, 301): 437, (100, 265): 438, (265, 256): 439, (115, 117): 440, (108, 32): 441, (115, 270): 442, (105, 118): 443, (116, 97): 444, (112, 435): 445, (65, 114): 446, (278, 256): 447, (282, 294): 448, (282, 275): 449, (102, 274): 450, (108, 260): 451, (112, 111): 452, (378, 32): 453, (65, 363): 454, (420, 266): 455, (275, 256): 456, (349, 260): 457, (428, 266): 458, (97, 121): 459, (115, 112): 460, (101, 112): 461, (285, 262): 462, (454, 258): 463, (101, 109): 464, (266, 269): 465, (403, 292): 466, (281, 352): 467, (119, 274): 468, (264, 115): 469, (284, 263): 470, (290, 110): 471, (116, 280): 472, (115, 308): 473, (402, 360): 474, (290, 451): 475, (107, 32): 476, (116, 119): 477, (105, 109): 478, (446, 430): 479, (39, 32): 480, (68, 105): 481, (101, 278): 482, (463, 294): 483, (121, 263): 484, (290, 266): 485, (281, 72): 486, (282, 109): 487, (305, 105): 488, (339, 275): 489, (112, 32): 490, (354, 271): 491, (97, 98): 492, (372, 115): 493, (277, 109): 494, (276, 100): 495, (375, 264): 496, (311, 469): 497, (291, 266): 498, (101, 266): 499, (97, 271): 500, (116, 301): 501, (276, 99): 502, (258, 297): 503, (101, 97): 504, (76, 300): 505, (273, 442): 506, (261, 280): 507, (114, 290): 508, (445, 375): 509, (380, 32): 510, (293, 104): 511, (117, 313): 512}\n",
            "ori tokens lenght: 42560\n",
            "new tokens lenght: 18677\n",
            "compression ratio: 2.28X\n",
            "Building vocabulary...\n",
            "Complete vocab:  {0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'e ', 257: b's ', 258: b'th', 259: b'n ', 260: b'd ', 261: b'at', 262: b'the ', 263: b', ', 264: b'an', 265: b'er', 266: b't ', 267: b'in', 268: b'f ', 269: b'of ', 270: b'o ', 271: b'y ', 272: b're', 273: b'al', 274: b'or', 275: b'on', 276: b'en', 277: b'hi', 278: b'ar', 279: b'es ', 280: b'ed ', 281: b'. ', 282: b'ti', 283: b'and ', 284: b'es', 285: b'to ', 286: b'a ', 287: b'in ', 288: b'\\n\\n', 289: b'ing', 290: b'ou', 291: b'os', 292: b'ch', 293: b'is', 294: b'on ', 295: b'om', 296: b'ing ', 297: b'is ', 298: b'he ', 299: b'as ', 300: b'ov', 301: b'er ', 302: b'ic', 303: b'at ', 304: b'lat', 305: b'cr', 306: b'as', 307: b'en ', 308: b'el', 309: b'um', 310: b'di', 311: b'tr', 312: b'mp', 313: b'al ', 314: b'an ', 315: b'op', 316: b'ci', 317: b'that ', 318: b'his ', 319: b'wh', 320: b'pe', 321: b'th ', 322: b'be', 323: b'ad', 324: b'ocr', 325: b'it', 326: b'ocrat', 327: b'Socrat', 328: b'ir', 329: b'lov', 330: b'ur', 331: b'of the ', 332: b'ow', 333: b'ac', 334: b'us', 335: b'\\n\\n\\n', 336: b'or ', 337: b'ex', 338: b'ol', 339: b'Plat', 340: b'ent', 341: b'st', 342: b'wi', 343: b'con', 344: b'osi', 345: b'ly ', 346: b'.\\n\\n\\n', 347: b'ch ', 348: b'ymp', 349: b'ha', 350: b'ymposi', 351: b'de', 352: b'The ', 353: b'ymposium', 354: b'the', 355: b', and ', 356: b'was ', 357: b'\\xe2\\x80', 358: b'com', 359: b'ro', 360: b\"'s \", 361: b'bi', 362: b'le', 363: b'ga', 364: b'Socrates ', 365: b'by ', 366: b'spe', 367: b'li', 368: b'Al', 369: b'with ', 370: b'gu', 371: b'ati', 372: b'pre', 373: b'e of ', 374: b'us ', 375: b'oph', 376: b', the ', 377: b'am', 378: b'Symposium', 379: b'biad', 380: b'tic', 381: b'ed', 382: b've ', 383: b'Alci', 384: b'Alcibiad', 385: b'gh', 386: b'ter', 387: b'ic ', 388: b'ts ', 389: b'qu', 390: b'ev', 391: b'un', 392: b'in the ', 393: b'od', 394: b'be ', 395: b'om ', 396: b'fr', 397: b'ag', 398: b'au', 399: b'it ', 400: b'bl', 401: b'hil', 402: b'Plato', 403: b'spee', 404: b'e. ', 405: b'ma', 406: b'm ', 407: b'ed the ', 408: b'si', 409: b'In ', 410: b'e, ', 411: b'for ', 412: b'who ', 413: b'ul', 414: b'ent ', 415: b'dial', 416: b's, ', 417: b'par', 418: b'des', 419: b'ogu', 420: b'no', 421: b'love ', 422: b'whi', 423: b'dialogu', 424: b'cl', 425: b'se', 426: b'ec', 427: b'which ', 428: b'bu', 429: b'pl', 430: b'ist', 431: b'from ', 432: b'pr', 433: b'Alcibiades ', 434: b'ce ', 435: b'hilos', 436: b'ation ', 437: b'ther ', 438: b'der', 439: b'ere ', 440: b'su', 441: b'l ', 442: b'so ', 443: b'iv', 444: b'ta', 445: b'philos', 446: b'Ar', 447: b'are ', 448: b'tion ', 449: b'tion', 450: b'for', 451: b'ld ', 452: b'po', 453: b'Symposium ', 454: b'Aga', 455: b'not ', 456: b'one ', 457: b'had ', 458: b'but ', 459: b'ay', 460: b'sp', 461: b'ep', 462: b'to the ', 463: b'Agath', 464: b'em', 465: b't of ', 466: b'speech', 467: b'. The ', 468: b'wor', 469: b'ans', 470: b'es, ', 471: b'oun', 472: b'ted ', 473: b'sel', 474: b\"Plato's \", 475: b'ould ', 476: b'k ', 477: b'tw', 478: b'im', 479: b'Arist', 480: b\"' \", 481: b'Di', 482: b'ear', 483: b'Agathon ', 484: b'y, ', 485: b'out ', 486: b'. H', 487: b'tim', 488: b'cri', 489: b'Platon', 490: b'p ', 491: b'they ', 492: b'ab', 493: b'pres', 494: b'him', 495: b'end', 496: b'ophan', 497: b'trans', 498: b'ost ', 499: b'et ', 500: b'ay ', 501: b'ter ', 502: b'enc', 503: b'this ', 504: b'ea', 505: b'Lov', 506: b'also ', 507: b'ated ', 508: b'rou', 509: b'philosoph', 510: b'tic ', 511: b'ish', 512: b'ual '}\n"
          ]
        }
      ],
      "source": [
        "import BasicTokenizer as BT\n",
        "tokenizer = BT.BasicTokenizer()\n",
        "\n",
        "training_text = \"\"\n",
        "with open('Symposium_Plato.txt', 'r', encoding = 'utf-8') as f:\n",
        "  training_text = f.read()\n",
        "\n",
        "tokenizer.train(training_text, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded tokens: [409, 262, 378, 263, 69, 359, 257, 297, 272, 99, 111, 103, 110, 105, 122, 280, 98, 111, 321, 299, 265, 111, 510, 329, 301, 283, 299, 286, 112, 104, 276, 295, 276, 294, 99, 97, 112, 97, 400, 373, 267, 460, 328, 296, 99, 290, 114, 397, 410, 118, 273, 274, 263, 103, 272, 303, 351, 381, 257, 283, 468, 107, 115]\n",
            "Decoded text: ['In ', 'the ', 'Symposium', ', ', 'E', 'ro', 's ', 'is ', 're', 'c', 'o', 'g', 'n', 'i', 'z', 'ed ', 'b', 'o', 'th ', 'as ', 'er', 'o', 'tic ', 'lov', 'er ', 'and ', 'as ', 'a ', 'p', 'h', 'en', 'om', 'en', 'on ', 'c', 'a', 'p', 'a', 'bl', 'e of ', 'in', 'sp', 'ir', 'ing ', 'c', 'ou', 'r', 'ag', 'e, ', 'v', 'al', 'or', ', ', 'g', 're', 'at ', 'de', 'ed', 's ', 'and ', 'wor', 'k', 's']\n"
          ]
        }
      ],
      "source": [
        "# import BasicTokenizer as BT\n",
        "# tokenizer = BT.BasicTokenizer()\n",
        "my_text = \"In the Symposium, Eros is recognized both as erotic lover and as a phenomenon capable of inspiring courage, valor, great deeds and works\"\n",
        "my_tokens = tokenizer.encode(my_text)\n",
        "print('Encoded tokens:', my_tokens)\n",
        "print('Decoded text:', tokenizer.decode(my_tokens))\n",
        "# ''.join(decoded_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO8/IfU0jIR7EPtaCLlDPGA",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
